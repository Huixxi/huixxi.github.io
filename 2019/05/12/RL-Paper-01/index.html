<!doctype html>



  


<html class="theme-next pisces use-motion">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/VEN/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/VEN/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Reinforcement Learning,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2">






<meta name="description" content="Aerobatics Control of Flying Creatures via Self-Regulated LearningPaper SourceJournal: ACM Transactions on GraphicsYear: 2018#Physics-Based Controller #Deep Reinforcement Learning(DRL) #Self-Regulated">
<meta name="keywords" content="Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="RL_Paper_01">
<meta property="og:url" content="http://cycygogo.cn/2019/05/12/RL-Paper-01/index.html">
<meta property="og:site_name" content="HU">
<meta property="og:description" content="Aerobatics Control of Flying Creatures via Self-Regulated LearningPaper SourceJournal: ACM Transactions on GraphicsYear: 2018#Physics-Based Controller #Deep Reinforcement Learning(DRL) #Self-Regulated">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-05-14T02:10:49.690Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL_Paper_01">
<meta name="twitter:description" content="Aerobatics Control of Flying Creatures via Self-Regulated LearningPaper SourceJournal: ACM Transactions on GraphicsYear: 2018#Physics-Based Controller #Deep Reinforcement Learning(DRL) #Self-Regulated">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://cycygogo.cn/2019/05/12/RL-Paper-01/">


  <title> RL_Paper_01 | HU </title>
</head>

<body itemscope itemtype="//schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="//schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">HU</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">\&</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="//schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                RL_Paper_01
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2019-05-12T19:44:54+08:00" content="2019-05-12">
              2019-05-12
            </time>
          </span>

          
            <span class="post-category">
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2019/05/12/RL-Paper-01/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/12/RL-Paper-01/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Aerobatics-Control-of-Flying-Creatures-via-Self-Regulated-Learning"><a href="#Aerobatics-Control-of-Flying-Creatures-via-Self-Regulated-Learning" class="headerlink" title="Aerobatics Control of Flying Creatures via Self-Regulated Learning"></a>Aerobatics Control of Flying Creatures via Self-Regulated Learning</h2><p><em><a href="http://mrl.snu.ac.kr/research/ProjectAerobatics/Aerobatics.htm" target="_blank" rel="noopener">Paper Source</a></em><br><code>Journal: ACM Transactions on Graphics</code><br><code>Year: 2018</code><br><code>#</code><em>Physics-Based Controller</em> <code>#</code><em>Deep Reinforcement Learning(DRL)</em> <code>#</code><em>Self-Regulated Learning</em><br><u><p align="center">Abstract</p></u>  <strong><em>Self-Regulated Learning (SRL)</em></strong>, which is combined with <strong><em>DRL</em></strong> to address the <strong><em>aerobatics control</em></strong> problem. The key idea of <strong><em>SRL</em></strong> is to allow the agent to take control over its own learning using an additional <strong><em>self-regulation policy</em></strong>. The policy allows the agent to regulate its goals according to the capability of the current control policy. The control and self-regulation policies are learned jointly along the progress of learning. Self-regulated learning can be viewed as building its own curriculum and seeking compromise on the goals.<br><a id="more"></a></p>
<blockquote>
<p>自调节学习(SRL)，与DRL相结合，解决特技飞行控制问题。SRL的关键思想是允许代理(agent)使用额外的自我调节政策来控制自己的学习。该政策允许代理根据当前控制政策的能力来调节其目标。控制和自我调节政策是沿着学习的过程共同学习的。自我调节学习可以被视为建立自己的课程并寻求对目标的妥协。</p>
</blockquote>
<p><u>1.INTRODUCTION</u><br>Defining a reward for taking an action is the primary means by which the user can influence the control policy. The reward is a succinct description of the task. The choice of the reward also affects the performance of the controller and the progress of its learning. </p>
<p>We consider a class of problems in which <u>the main goal can be achieved by generating a sequence of subgoals and addressing each individual subgoal sequentially.</u></p>
<blockquote>
<p>定义采取行动的奖励是用户可以影响控制策略的主要手段。奖励是对任务的简洁描述。奖励的选择也会影响控制器的性能和学习的进度。<br>我们考虑这一类特定问题，其主要目标可以通过生成一系列子目标并按顺序处理每个单独的子目标来实现。</p>
</blockquote>
<p><u>2.RELATED WORK</u><br><strong><em><a href="https://www.thoughtco.com/principle-of-optimality-definition-1147078" target="_blank" rel="noopener">Optimality principles</a></em></strong> played an important role of popularizing optimal control theory and nonlinear/non-convex optimization methods in character animation. We can classify control policies (a.k.a., controllers) into <strong><em>immediate control</em></strong> and <strong><em>predictive control</em></strong> depending on how far they look ahead into their future. </p>
<blockquote>
<p><code>Optimality principles:</code> The principle of optimality is the basic principle of dynamic programming, which was developed by Richard Bellman: that an optimal path has the property that whatever the initial conditions and control variables (choices) over some initial period, the control (or decision variables) chosen over the remaining period must be optimal for the remaining problem, with the state resulting from the early decisions taken to be the initial condition. </p>
</blockquote>
<p><code>Immediate control policy:</code> A direct mapping from states to actions.<br><code>Model predictive control:</code> The key concept is to predict the future evolution of the dynamic system for short time horizon and optimize its control signals. Model predictive control repeats this prediction and optimization step while receding the time horizon. </p>
<p>Recently, Deep Reinforcement Learning (DRL) has shown its potential in simulation and control of virtual characters. DRL for continuous control (especially <strong><em>actor-critic framework</em></strong>[<a href="http://busoniu.net/files/papers/ivo_smcc12_survey.pdf" target="_blank" rel="noopener">survey</a>]) has advantages of both immediate control and predictive control. </p>
<p>SRL assumes that the reward function provided initially is not ideal for achieving the main goal and thus allows the agent to transform the reward function adaptively. Underlying motivation of SRL is closely related to <strong><em><a href="https://www.ijcai.org/proceedings/2017/0757.pdf" target="_blank" rel="noopener">automatic curriculum generation</a></em></strong> for RL agents. </p>
<blockquote>
<p><strong><em>最优性原理</em></strong>在推广最优控制理论和角色动画中的非线性/非凸优化方法起到了重要作用。我们可以将控制策略（也就是控制器）分类，根据它能够对未来展望多远分为<strong><em>立即控制</em></strong>和<strong><em>预测控制</em></strong>。　</p>
<blockquote>
<p><strong><em>最优性原理：</em></strong>最优性原理是动态规划的基本原理，由R.Bellman等人通过研究一类多阶段决策问题提出，“作为整个过程的最优策略，无论过去的状态和决策如何，对前面的决策形成的状态而言，余下的诸决策必构成最优策略。”　简言之，一个最优策略的子策略总是最优的。　</p>
</blockquote>
<p><code>立即控制策略：</code>从状态到动作的直接映射。<br><code>模型预测控制：</code>关键概念是预测短期内动态系统的未来发展，优化其控制信号。模型预测控制在时间范围后退的同时重复这一预测和优化步骤。<br>近年来，深度强化学习（DRL）在虚拟人物的仿真和控制方面显示出其潜力。连续控制的DRL（尤其是<strong><em>演员－批评家框架</em></strong>）具有立即控制和预测控制的优点。<br>SRL假设最初提供的奖励函数不适合实现主要目标，因此允许代理自适应地转换奖励函数。SRL的潜在动机与RL代理的<strong><em>课程自动生成</em></strong>密切相关。</p>
</blockquote>
<p><u>3.ENVIRONMENT</u><br>There refers to some basic specific knowledge about physics and physics control. So just understand the concepts of these definations and symbols to the degree of understanding the following algorithms without any troubles.</p>
<blockquote>
<p><code>Trajectory</code> $C(\sigma) = (R(\sigma), p(\sigma), h(\sigma))$ </p>
<blockquote>
<p><code>Progress Parameter</code> $\sigma \in [0, 1]$<br><code>Orientation</code> $R(\sigma) \in SO(3)$ <a href="https://www.quora.com/What-are-SU-2-SO-3-and-other-similar-names-in-physics" target="_blank" rel="noopener">What SO(3) is?</a><br><code>Position</code> $p(\sigma) \in R^3$<br><code>Clearance Threshold</code> $h(\sigma)$ </p>
</blockquote>
<p><code>Distance</code> <script type="math/tex">d(R, p, \sigma) = ||\log(R^{-1}R(\sigma))||^2_F + w_p||p − p(\sigma)||^2</script><br>if $d(R, p, \sigma^*) &lt; h(\sigma^*)$, then $C(\sigma^*)$ is cleared.<br><code>Unit tangent vector</code> $t(\sigma) = \frac{\dot{p(\sigma)}}{||\dot{p(\sigma)}||}$<br><code>Up-Vector</code> $u = [0, 1, 0]$<br><code>Initial Orientation</code> $R(0) = [r^T_x, r^T_y, r^T_z] \in SO(3)$ is defined by an orthogonal frame such that</p>
<blockquote>
<p>$r_z = t(0)$,<br>$r_x = \frac{u×r_z}{||u×r_z||}$<br>$r_y = r_z × r_x$.</p>
</blockquote>
<p><code>Trajectory Rotation</code> $R(\sigma) = R(\sigma − \epsilon)U(t(\sigma − \epsilon), t(\sigma))$<br><code>Minimal Rotation between two vectors a and b</code><br>$U(a, b) = I + [a × b]_× + [a × b]^2_× \frac{1 − a·b}{(a × b)^T(a × b)}$<br>$[v]_×$ is the skew-symmetric cross-product matrix of $v$.<br><code>Relaxed Clearance Threshold</code> $h(\sigma) = \bar{h}(1 + w_h ||\ddot{p}(\sigma)||)$<br><code>Reward defination</code></p>
<script type="math/tex; mode=display">R(s, a, s′)=\left\{
    \begin{array}{ll}
      \sigma^*(2 − \frac{d(R,p,\sigma^*)}{d_{max}}), if d(R,p,\sigma^*) < h(\sigma^*) \\
      0, otherwise
    \end{array}
  \right.</script></blockquote>
<p><u>4.ALGORITHM</u><br><u>DRL Algorithm</u><br>That algorithm is similar with <strong><em>actor-critic framework</em></strong>, it also has two deep neural networks, one called $Q$, represents <em>state-action value function</em>, state-action pair $(s, a)$ as input and returns the expectation on cumulative rewards. Another called $\pi$The deterministic policy $\pi$ takes state $s$ as input and generates action $a$.<br>It consists of two parts:</p>
<ul>
<li>Generating a seuqence of $e_i = (s_{i−1}, a_i, r_i, s_i)$ tuples to simulate the given trajectory.</li>
<li>Using a single sample of the former sequence to update the state-action value function and policy function repetedly.</li>
</ul>
<p><u>Self-Regulated DRL</u><br>Intuitively speaking, the agent senses its body state $s_d$ and a part of the trajectory $(\sigma, s_s)$, and decides how to act and how to regulate the current subgoal $C(\sigma)$ simultaneously while avoiding excessive deviation from the input trajectory.</p>
<p><u>Experts in related field</u><br><a href="https://xbpeng.github.io/" target="_blank" rel="noopener">Jason Peng</a><br><a href="http://libliu.com/" target="_blank" rel="noopener">Libin Liu</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning/" rel="tag">#Reinforcement Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2062/01/31/Navigation/" rel="prev" title="Navigation">
                Navigation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/Elon-Musk.jpg" alt="Hugo">
          <p class="site-author-name" itemprop="name">Hugo</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Huixxi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/uestc-hugo/" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="uestc.hugo@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/Hugo13438832589" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Aerobatics-Control-of-Flying-Creatures-via-Self-Regulated-Learning"><span class="nav-number">1.</span> <span class="nav-text">Aerobatics Control of Flying Creatures via Self-Regulated Learning</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hugo</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/VEN/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/VEN/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/VEN/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/VEN/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/VEN/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/VEN/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'hugo-818';
      var disqus_identifier = '2019/05/12/RL-Paper-01/';
      var disqus_title = "RL_Paper_01";
      var disqus_url = 'http://cycygogo.cn/2019/05/12/RL-Paper-01/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
